{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyes0kkiMe4W9phT2dfdwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akint980/Vegamour_Data_Analysis/blob/main/Tweetlens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generating Tweets related to Vegamour Hair products in order to gain insights into Customers' feedback. Customers' feedback can be used to improve products. To generate these data, I made use of Synthetic tweets from Transformers and real tweets from X using X API. This code produced real tweets from X using X bearer token."
      ],
      "metadata": {
        "id": "fVmXOYeW6wyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1yAACCLYVNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aeaba6f-b5b0-48cb-8732-14dc5138ee8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 6 real tweets to vegamour_real_tweets.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import tweepy\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 🔐 Hardcoded X API Bearer Token\n",
        "# -----------------------------\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAHlD3AEAAAAAY%2BdZ8XM23sb3kEOmmxTH9rcnlEk%3DwalutA7bG4jn7ISxA3ba04EseoEuJu2fkSENi42WZrPXfyEDPm\"\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 🧼 Clean tweet text\n",
        "# -----------------------------\n",
        "def clean_tweet(text):\n",
        "    text = re.sub(r'https?://\\S+', '', text)        # Remove URLs\n",
        "    text = re.sub(r'[@#]\\w+', '', text)             # Remove mentions/hashtags\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()        # Normalize whitespace\n",
        "    return text[:277] + \"...\" if len(text) > 280 else text\n",
        "\n",
        "# -----------------------------\n",
        "# 📦 Metadata Pools\n",
        "# -----------------------------\n",
        "products = [\"Vegamour GRO Serum\", \"Vegamour Biotin Gummies\", \"Vegamour Scalp Detox\", \"Vegamour Dry Shampoo\"]\n",
        "emotions = [\"Joy\", \"Frustration\", \"Nostalgia\", \"Gratitude\"]\n",
        "intents = [\"Praise\", \"Complaint\", \"Testimonial\", \"Recommendation\"]\n",
        "\n",
        "# -----------------------------\n",
        "# 🔍 Fetch real tweets from X\n",
        "# -----------------------------\n",
        "def fetch_vegamour_tweets(query=\"vegamour\", max_results=300):\n",
        "    tweets_data = []\n",
        "    response = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        max_results=100,  # max per call\n",
        "        tweet_fields=[\"created_at\", \"text\"],\n",
        "    )\n",
        "\n",
        "    if not response.data:\n",
        "        print(\"No tweets found.\")\n",
        "        return []\n",
        "\n",
        "    for idx, tweet in enumerate(response.data):\n",
        "        cleaned = clean_tweet(tweet.text)\n",
        "        timestamp = tweet.created_at.isoformat()\n",
        "\n",
        "        tweets_data.append([\n",
        "            idx + 1,  # prompt_id replacement\n",
        "            timestamp,\n",
        "            cleaned,\n",
        "            random.choice(products),\n",
        "            random.choice(emotions),\n",
        "            random.choice(intents)\n",
        "        ])\n",
        "\n",
        "    return tweets_data\n",
        "\n",
        "# -----------------------------\n",
        "# 💾 Save tweets to CSV\n",
        "# -----------------------------\n",
        "def save_to_csv(rows, output_file=\"vegamour_real_tweets.csv\"):\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"prompt_id\", \"timestamp\", \"tweet_text\", \"product_mention\", \"emotion_tag\", \"intent_tag\"])\n",
        "        writer.writerows(rows)\n",
        "    print(f\"✅ Saved {len(rows)} real tweets to {output_file}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 🚀 Main Execution\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    real_tweets = fetch_vegamour_tweets()\n",
        "    if real_tweets:\n",
        "        save_to_csv(real_tweets)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkHiHvOb3Zkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aeaba6f-b5b0-48cb-8732-14dc5138ee8d",
        "id": "S2Cv8l-73dbm"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 6 real tweets to vegamour_real_tweets.csv\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Generating tweets using transformers."
      ],
      "metadata": {
        "id": "VY5wi7_cE7Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 model for text generation\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Metadata pools\n",
        "products = [\"Vegamour GRO Serum\", \"Vegamour Biotin Gummies\", \"Vegamour Scalp Detox\", \"Vegamour Dry Shampoo\"]\n",
        "emotions = [\"Joy\", \"Frustration\", \"Nostalgia\", \"Gratitude\"]\n",
        "intents = [\"Praise\", \"Complaint\", \"Testimonial\", \"Recommendation\"]\n",
        "\n",
        "# Cleaning function for generated tweets\n",
        "def clean_tweet(text):\n",
        "    # Remove GPT prompt echoes or irrelevant text\n",
        "    text = re.sub(r'Tweet\\s*#\\d+\\s*about\\s*Vegamour[:\\-]*\\s*', '', text, flags=re.IGNORECASE)\n",
        "    # Remove URLs, hashtags, mentions\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(r'[@#]\\w+', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Limit length (max 280 chars for tweets)\n",
        "    if len(text) > 280:\n",
        "        text = text[:277] + \"...\"\n",
        "    return text\n",
        "\n",
        "# Generate tweets function\n",
        "def generate_tweets(prompt, num_tweets):\n",
        "    results = generator(\n",
        "        prompt,\n",
        "        max_length=60,\n",
        "        num_return_sequences=num_tweets,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    return [clean_tweet(r[\"generated_text\"]) for r in results]\n",
        "\n",
        "# Generate synthetic Vegamour tweets\n",
        "all_rows = []\n",
        "num_prompts = 30\n",
        "tweets_per_prompt = 30\n",
        "start_time = datetime(2024, 7, 1, 8, 0, 0)  # Changed year to 2020\n",
        "\n",
        "for prompt_id in range(1, num_prompts + 1):\n",
        "    prompt = f\"Vegamour hair product tweet #{prompt_id}\"\n",
        "    synthetic_tweets = generate_tweets(prompt, tweets_per_prompt)\n",
        "\n",
        "    for i, tweet_text in enumerate(synthetic_tweets):\n",
        "        timestamp = start_time + timedelta(minutes=prompt_id * 50 + i)\n",
        "        all_rows.append([\n",
        "            prompt_id,\n",
        "            timestamp.isoformat(),\n",
        "            tweet_text,\n",
        "            random.choice(products),\n",
        "            random.choice(emotions),\n",
        "            random.choice(intents)\n",
        "        ])\n",
        "\n",
        "# Save generated tweets to CSV\n",
        "output_file = \"vegamour_hair_product_tweets_2024.csv\"\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"prompt_id\", \"timestamp\", \"tweet_text\", \"product_mention\", \"emotion_tag\", \"intent_tag\"])\n",
        "    writer.writerows(all_rows)\n",
        "\n",
        "print(f\"✅ Saved {len(all_rows)} Vegamour hair product tweets to {output_file}\")\n"
      ],
      "metadata": {
        "id": "4b63p7roCmM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}